<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gesture Recognition System - Osinachi Mbakamma | Embedded AI Research</title>
    <meta name="description" content="Real-time Hand Gesture Recognition for Robotic Control - Embedded AI Project">
    <link rel="stylesheet" href="../assets/css/style.css">
    <style>
        .project-hero {
            background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%);
            color: white;
            padding: 6rem 0 4rem;
            margin-top: 80px;
            text-align: center;
        }
        .project-content {
            max-width: 1000px;
            margin: 0 auto;
            padding: 3rem 2rem;
        }
        .project-meta {
            display: flex;
            gap: 2rem;
            margin: 2rem 0;
            flex-wrap: wrap;
        }
        .meta-card {
            background: white;
            padding: 1.5rem;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            flex: 1;
            min-width: 200px;
        }
        .performance-metrics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }
        .metric-card {
            background: white;
            padding: 1.5rem;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            text-align: center;
        }
        .metric-value {
            font-size: 1.8rem;
            font-weight: 800;
            color: var(--secondary-color);
            margin-bottom: 0.5rem;
        }
        .methodology-step {
            background: white;
            padding: 2rem;
            margin: 1.5rem 0;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            border-left: 4px solid #f59e0b;
        }
        .comparison-table {
            overflow-x: auto;
            margin: 1.5rem 0;
        }
        .tech-stack {
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
            margin: 1rem 0;
        }
        .tech-tag {
            background: #f59e0b;
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 20px;
            font-size: 0.9rem;
        }
        .system-architecture {
            background: #f8f9fa;
            padding: 2rem;
            border-radius: 10px;
            margin: 2rem 0;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div class="logo">
                    <h1>Osinachi Mbakamma</h1>
                    <p class="subtitle">AI Research Scientist</p>
                </div>
                <nav>
                    <a href="../index.html">Home</a>
                    <a href="../research.html">Research</a>
                    <a href="../projects.html">Projects</a>
                    <a href="../publications.html">Publications</a>
                    <a href="../cv.html">CV</a>
                    <a href="../contact.html">Contact</a>
                </nav>
            </div>
        </div>
    </header>

    <!-- Project Hero -->
    <section class="project-hero">
        <div class="container">
            <h1>Real-time Hand Gesture Recognition for Robotic Control</h1>
            <p>Advanced computer vision system enabling intuitive human-robot interaction through gesture-based commands</p>
            <div class="project-meta">
                <div class="meta-card">
                    <strong>Status</strong>
                    <div>Completed</div>
                </div>
                <div class="meta-card">
                    <strong>Domain</strong>
                    <div>Computer Vision & Robotics</div>
                </div>
                <div class="meta-card">
                    <strong>Application</strong>
                    <div>Human-Robot Interaction</div>
                </div>
                <div class="meta-card">
                    <strong>Institution</strong>
                    <div>Wroclaw University of Science and Technology</div>
                </div>
            </div>
        </div>
    </section>

    <main class="project-content">
        <!-- Abstract -->
        <section>
            <h2>Abstract</h2>
            <p>Human-robot interaction represents a critical frontier in robotics research, with gesture-based control offering intuitive and natural communication between humans and robotic systems. This project developed a comprehensive hand gesture recognition system capable of real-time interpretation of human gestures for robotic arm control.</p>
            
            <p>The research compared RGB and RGB-D (depth) based algorithms for gesture recognition, evaluating their performance across accuracy, computational efficiency, and robustness to environmental variations. By leveraging advanced computer vision techniques and deep learning architectures, the system achieves high-precision gesture classification suitable for real-world robotic applications.</p>
            
            <p>Key innovations include optimized model architectures for embedded deployment, real-time processing capabilities, and robust performance under varying lighting conditions and backgrounds. The system demonstrates the practical feasibility of vision-based gesture control for industrial, healthcare, and service robotics applications.</p>
        </section>

        <!-- Performance Metrics -->
        <section style="margin: 3rem 0;">
            <h2>System Performance</h2>
            <div class="performance-metrics">
                <div class="metric-card">
                    <div class="metric-value">95.2%</div>
                    <div>Overall Accuracy</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">24ms</div>
                    <div>Inference Time</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">15 FPS</div>
                    <div>Processing Rate</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">12</div>
                    <div>Supported Gestures</div>
                </div>
            </div>
        </section>

        <!-- System Architecture -->
        <section class="system-architecture">
            <h2>System Architecture</h2>
            <p>The gesture recognition pipeline consists of multiple integrated components:</p>
            
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.5rem; margin: 1.5rem 0;">
                <div style="background: white; padding: 1.5rem; border-radius: 10px; text-align: center;">
                    <h4>üì∑ Input Acquisition</h4>
                    <p>RGB and depth data capture from camera systems</p>
                </div>
                <div style="background: white; padding: 1.5rem; border-radius: 10px; text-align: center;">
                    <h4>‚úã Hand Detection</h4>
                    <p>Robust hand localization and segmentation</p>
                </div>
                <div style="background: white; padding: 1.5rem; border-radius: 10px; text-align: center;">
                    <h4>üîç Feature Extraction</h4>
                    <p>Spatial and temporal feature representation</p>
                </div>
                <div style="background: white; padding: 1.5rem; border-radius: 10px; text-align: center;">
                    <h4>üß† Gesture Classification</h4>
                    <p>Deep learning-based gesture recognition</p>
                </div>
            </div>
        </section>

        <!-- Technical Implementation -->
        <section style="margin: 3rem 0;">
            <h2>Technical Implementation</h2>
            
            <h3>Tools & Technologies</h3>
            <div class="tech-stack">
                <span class="tech-tag">Python</span>
                <span class="tech-tag">OpenCV</span>
                <span class="tech-tag">PyTorch</span>
                <span class="tech-tag">MediaPipe</span>
                <span class="tech-tag">ROS</span>
                <span class="tech-tag">TensorFlow</span>
                <span class="tech-tag">CUDA</span>
                <span class="tech-tag">Linux</span>
            </div>

            <h3>Algorithm Comparison</h3>
            <div class="comparison-table">
                <table style="width: 100%; border-collapse: collapse; background: white; border-radius: 10px; overflow: hidden; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    <thead style="background: #f59e0b; color: white;">
                        <tr>
                            <th style="padding: 1rem; text-align: left;">Algorithm</th>
                            <th style="padding: 1rem; text-align: center;">Accuracy</th>
                            <th style="padding: 1rem; text-align: center;">Speed</th>
                            <th style="padding: 1rem; text-align: center;">Robustness</th>
                            <th style="padding: 1rem; text-align: center;">Hardware Requirements</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 1rem; font-weight: 600;">CNN (RGB)</td>
                            <td style="padding: 1rem; text-align: center;">92.3%</td>
                            <td style="padding: 1rem; text-align: center;">Fast</td>
                            <td style="padding: 1rem; text-align: center;">Medium</td>
                            <td style="padding: 1rem; text-align: center;">Low</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 1rem; font-weight: 600;">3D CNN (RGB-D)</td>
                            <td style="padding: 1rem; text-align: center;">95.2%</td>
                            <td style="padding: 1rem; text-align: center;">Medium</td>
                            <td style="padding: 1rem; text-align: center;">High</td>
                            <td style="padding: 1rem; text-align: center;">High</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #eee;">
                            <td style="padding: 1rem; font-weight: 600;">LSTM + CNN</td>
                            <td style="padding: 1rem; text-align: center;">93.8%</td>
                            <td style="padding: 1rem; text-align: center;">Slow</td>
                            <td style="padding: 1rem; text-align: center;">High</td>
                            <td style="padding: 1rem; text-align: center;">Medium</td>
                        </tr>
                        <tr>
                            <td style="padding: 1rem; font-weight: 600;">Traditional CV</td>
                            <td style="padding: 1rem; text-align: center;">85.6%</td>
                            <td style="padding: 1rem; text-align: center;">Very Fast</td>
                            <td style="padding: 1rem; text-align: center;">Low</td>
                            <td style="padding: 1rem; text-align: center;">Very Low</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <!-- Methodology -->
        <section>
            <h2>Research Methodology</h2>
            
            <div class="methodology-step">
                <h3>1. Dataset Creation & Annotation</h3>
                <p>Comprehensive dataset development with diverse conditions:</p>
                <ul>
                    <li><strong>Data Collection:</strong> 50,000+ gesture samples across 12 different gesture classes</li>
                    <li><strong>Diversity:</strong> Multiple subjects, lighting conditions, backgrounds, and camera angles</li>
                    <li><strong>Annotation:</strong> Manual labeling of gesture boundaries and classifications</li>
                    <li><strong>Augmentation:</strong> Synthetic data generation through rotation, scaling, and lighting variations</li>
                </ul>
            </div>

            <div class="methodology-step">
                <h3>2. Model Architecture Design</h3>
                <p>Custom deep learning architectures optimized for gesture recognition:</p>
                
                <h4>RGB-Based CNN</h4>
                <ul>
                    <li>Input: 224x224 RGB images</li>
                    <li>Architecture: 5 convolutional layers with batch normalization</li>
                    <li>Activation: ReLU with dropout regularization</li>
                    <li>Output: 12-class softmax classification</li>
                </ul>
                
                <h4>RGB-D 3D CNN</h4>
                <ul>
                    <li>Input: 224x224x4 (RGB + Depth) volumetric data</li>
                    <li>Architecture: 3D convolutional layers with spatiotemporal processing</li>
                    <li>Feature fusion: Early and late fusion strategies</li>
                    <li>Output: Multi-modal gesture classification</li>
                </ul>
            </div>

            <div class="methodology-step">
                <h3>3. Real-time Optimization</h3>
                <p>Performance optimization for embedded deployment:</p>
                <ul>
                    <li><strong>Model Quantization:</strong> FP16 and INT8 precision for faster inference</li>
                    <li><strong>Architecture Search:</strong> Neural architecture search for optimal efficiency-accuracy tradeoff</li>
                    <li><strong>Hardware Acceleration:</strong> CUDA optimization and TensorRT deployment</li>
                    <li><strong>Memory Management:</strong> Efficient memory usage for embedded systems</li>
                </ul>
            </div>
        </section>

        <!-- Applications -->
        <section style="margin: 3rem 0;">
            <h2>Applications & Use Cases</h2>
            
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 2rem;">
                <div style="background: white; padding: 2rem; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    <h4>üè≠ Industrial Robotics</h4>
                    <p>Intuitive control of robotic arms in manufacturing environments, enabling human-robot collaboration without traditional interfaces.</p>
                </div>
                <div style="background: white; padding: 2rem; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    <h4>üè• Healthcare Assistance</h4>
                    <p>Surgical robotics control and rehabilitation device operation through natural hand gestures.</p>
                </div>
                <div style="background: white; padding: 2rem; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                    <h4>üéÆ Interactive Systems</h4>
                    <p>Gesture-based interfaces for smart environments, gaming, and virtual reality applications.</p>
                </div>
            </div>
        </section>

        <!-- Challenges & Solutions -->
        <section>
            <h2>Technical Challenges & Solutions</h2>
            
            <div style="background: #f0f9ff; padding: 2rem; border-radius: 10px; margin: 1.5rem 0;">
                <h4>Challenge: Real-time Performance</h4>
                <p><strong>Solution:</strong> Implemented model quantization, hardware acceleration, and optimized data pipelines to achieve 24ms inference time suitable for real-time applications.</p>
            </div>

            <div style="background: #f0f9ff; padding: 2rem; border-radius: 10px; margin: 1.5rem 0;">
                <h4>Challenge: Environmental Robustness</h4>
                <p><strong>Solution:</strong> Developed comprehensive data augmentation strategies and multi-modal fusion to maintain performance across varying lighting and background conditions.</p>
            </div>

            <div style="background: #f0f9ff; padding: 2rem; border-radius: 10px; margin: 1.5rem 0;">
                <h4>Challenge: Gesture Ambiguity</h4>
                <p><strong>Solution:</strong> Implemented temporal modeling with LSTM networks and context-aware classification to resolve ambiguous gesture patterns.</p>
            </div>
        </section>

        <!-- Future Work -->
        <section>
            <h2>Future Research Directions</h2>
            <ul>
                <li><strong>Multi-modal Fusion:</strong> Integrating voice commands and gaze tracking with gesture recognition</li>
                <li><strong>Adaptive Learning:</strong> Systems that learn and adapt to individual user gesture patterns</li>
                <li><strong>Explainable AI:</strong> Providing intuitive explanations for gesture recognition decisions</li>
                <li><strong>Edge Deployment:</strong> Optimizing for resource-constrained embedded devices</li>
                <li><strong>Security:</strong> Protecting against adversarial attacks on vision-based control systems</li>
            </ul>
        </section>

        <!-- Project Resources -->
        <section style="background: #f8f9fa; padding: 2rem; border-radius: 10px; margin: 3rem 0;">
            <h3>Project Documentation</h3>
            <div style="display: flex; gap: 1rem; flex-wrap: wrap; margin-top: 1rem;">
                <a href="https://github.com/osinachix/gesture-recognition" class="btn primary">View Source Code</a>
                <a href="../assets/pdf/gesture-recognition-thesis.pdf" class="btn secondary">Download Technical Paper</a>
                <a href="../publications.html" class="btn secondary">Related Publications</a>
            </div>
        </section>

        <!-- Navigation -->
        <section style="text-align: center; margin-top: 3rem; padding: 2rem; background: var(--light-color); border-radius: 10px;">
            <h3>Explore Other Research Projects</h3>
            <div style="display: flex; gap: 1rem; justify-content: center; flex-wrap: wrap; margin-top: 1rem;">
                <a href="ai-security.html" class="btn secondary">‚Üê AI Security Framework</a>
                <a href="../projects.html" class="btn primary">View All Projects</a>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-info">
                    <p>&copy; 2024 Osinachi Mbakamma. Advancing Human-Robot Interaction Research.</p>
                    <p>Wroclaw, Poland | osinachimbakamma@gmail.com</p>
                </div>
                <div class="social-links">
                    <a href="https://github.com/osinachix">GitHub</a>
                    <a href="https://linkedin.com/in/osinachimbakamma/">LinkedIn</a>
                    <a href="https://scholar.google.com/citations?user=YOUR_ID">Google Scholar</a>
                    <a href="mailto:osinachimbakamma@gmail.com">Email</a>
                </div>
            </div>
        </div>
    </footer>

    <script src="../assets/js/main.js"></script>
</body>
</html>
